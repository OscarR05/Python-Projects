{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HuggingFace) \"NLP\" Library Overview / Quick Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp is a lightweight and extensible library to easily share and load dataset and evaluation metrics, \n",
    "# already providing access to ~100 datasets and ~10 evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 105 datasets are available on HuggingFace AWS bucket: \n",
      "aeslc\n",
      "ai2_arc\n",
      "anli\n",
      "billsum\n",
      "blimp\n",
      "blog_authorship_corpus\n",
      "boolq\n",
      "break_data\n",
      "cfq\n",
      "civil_comments\n",
      "cmrc2018\n",
      "cnn_dailymail\n",
      "coarse_discourse\n",
      "com_qa\n",
      "commonsense_qa\n",
      "coqa\n",
      "cornell_movie_dialog\n",
      "cos_e\n",
      "cosmos_qa\n",
      "crime_and_punish\n",
      "csv\n",
      "definite_pronoun_resolution\n",
      "discofuse\n",
      "drop\n",
      "empathetic_dialogues\n",
      "eraser_multi_rc\n",
      "esnli\n",
      "event2Mind\n",
      "flores\n",
      "fquad\n",
      "gap\n",
      "germeval_14\n",
      "gigaword\n",
      "glue\n",
      "hansards\n",
      "hellaswag\n",
      "imdb\n",
      "jeopardy\n",
      "kor_nli\n",
      "lc_quad\n",
      "librispeech_lm\n",
      "lm1b\n",
      "math_dataset\n",
      "math_qa\n",
      "mlqa\n",
      "movie_rationales\n",
      "multi_news\n",
      "multi_nli\n",
      "multi_nli_mismatch\n",
      "newsroom\n",
      "openbookqa\n",
      "opinosis\n",
      "para_crawl\n",
      "qa4mre\n",
      "qangaroo\n",
      "qasc\n",
      "quarel\n",
      "quartz\n",
      "quoref\n",
      "race\n",
      "reclor\n",
      "reddit\n",
      "reddit_tifu\n",
      "scan\n",
      "scicite\n",
      "scientific_papers\n",
      "scifact\n",
      "sciq\n",
      "scitail\n",
      "sentiment140\n",
      "snli\n",
      "social_i_qa\n",
      "squad\n",
      "squad_it\n",
      "squad_v1_pt\n",
      "squad_v2\n",
      "super_glue\n",
      "ted_hrlr\n",
      "ted_multi\n",
      "tiny_shakespeare\n",
      "trivia_qa\n",
      "tydiqa\n",
      "webis/tl_dr\n",
      "wiki40b\n",
      "wiki_qa\n",
      "wiki_split\n",
      "wikihow\n",
      "wikipedia\n",
      "wikitext\n",
      "winogrande\n",
      "wiqa\n",
      "wmt14\n",
      "wmt15\n",
      "wmt16\n",
      "wmt17\n",
      "wmt18\n",
      "wmt19\n",
      "wmt_t2t\n",
      "x_stance\n",
      "xcopa\n",
      "xnli\n",
      "xquad\n",
      "xsum\n",
      "xtreme\n",
      "yelp_polarity\n",
      "\n",
      "Currently 10 metrics are available on HuggingFace AWS bucket: \n",
      "bleu\n",
      "coval\n",
      "gleu\n",
      "glue\n",
      "rouge\n",
      "sacrebleu\n",
      "seqeval\n",
      "squad\n",
      "squad_v2\n",
      "xnli\n"
     ]
    }
   ],
   "source": [
    "# Currently available datasets and metrics\n",
    "datasets = nlp.list_datasets()\n",
    "metrics = nlp.list_metrics()\n",
    "\n",
    "print(f\"Currently {len(datasets)} datasets are available on HuggingFace AWS bucket: \\n\" \n",
    "      + '\\n'.join(dataset.id for dataset in datasets) + '\\n')\n",
    "print(f\"Currently {len(metrics)} metrics are available on HuggingFace AWS bucket: \\n\" \n",
    "      + '\\n'.join(metric.id for metric in metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ id: squad_v2\n",
      "ðŸ‘‰ key: nlp/datasets/squad_v2/squad_v2.py\n",
      "ðŸ‘‰ lastModified: 2020-05-14T14:57:28.000Z\n",
      "ðŸ‘‰ description: \\\n",
      "combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers\n",
      " to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but \n",
      " also determine when no answer is supported by the paragraph and abstain from answering.\n",
      "ðŸ‘‰ citation: \\\n",
      "@article{2016arXiv160605250R,\n",
      "       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
      "                 Konstantin and {Liang}, Percy},\n",
      "        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
      "      journal = {arXiv e-prints},\n",
      "         year = 2016,\n",
      "          eid = {arXiv:1606.05250},\n",
      "        pages = {arXiv:1606.05250},\n",
      "archivePrefix = {arXiv},\n",
      "       eprint = {1606.05250},\n",
      "}\n",
      "ðŸ‘‰ size: 4826\n",
      "ðŸ‘‰ etag: \"bd081793fbf5c8f899602b274b3caf16\"\n",
      "ðŸ‘‰ siblings: [{'key': 'nlp/datasets/squad_v2/dataset_infos.json', 'etag': '\"8b4b4328056e71954f0fed54cc7bba71\"', 'lastModified': '2020-05-14T15:43:09.000Z', 'size': 2225, 'rfilename': 'dataset_infos.json'}, {'key': 'nlp/datasets/squad_v2/dummy/1.0.0/dummy_data.zip', 'etag': '\"9996dc92bb9bb1470677a74642a4b876\"', 'lastModified': '2020-05-14T14:57:27.000Z', 'size': 14198, 'rfilename': 'dummy/1.0.0/dummy_data.zip'}, {'key': 'nlp/datasets/squad_v2/dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/dev', 'etag': '\"6e62fda3aa1bdbd1f389c4836e3ca21b\"', 'lastModified': '2020-05-14T14:57:27.000Z', 'size': 1488, 'rfilename': 'dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/dev'}, {'key': 'nlp/datasets/squad_v2/dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/dev-v2.0.json', 'etag': '\"6e62fda3aa1bdbd1f389c4836e3ca21b\"', 'lastModified': '2020-05-14T14:57:27.000Z', 'size': 1488, 'rfilename': 'dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/dev-v2.0.json'}, {'key': 'nlp/datasets/squad_v2/dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/train', 'etag': '\"6e62fda3aa1bdbd1f389c4836e3ca21b\"', 'lastModified': '2020-05-14T14:57:28.000Z', 'size': 1488, 'rfilename': 'dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/train'}, {'key': 'nlp/datasets/squad_v2/dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/train-v2.0.json', 'etag': '\"6e62fda3aa1bdbd1f389c4836e3ca21b\"', 'lastModified': '2020-05-14T14:57:28.000Z', 'size': 1488, 'rfilename': 'dummy/squad_v2/2.0.0/dummy_data-zip-extracted/dummy_data/train-v2.0.json'}, {'key': 'nlp/datasets/squad_v2/dummy/squad_v2/2.0.0/dummy_data.zip', 'etag': '\"98a2d3435fad9cd8d91b55cabd9c4c1b\"', 'lastModified': '2020-05-14T14:57:28.000Z', 'size': 3364, 'rfilename': 'dummy/squad_v2/2.0.0/dummy_data.zip'}, {'key': 'nlp/datasets/squad_v2/squad_v2.py', 'etag': '\"bd081793fbf5c8f899602b274b3caf16\"', 'lastModified': '2020-05-14T14:57:28.000Z', 'size': 4826, 'rfilename': 'squad_v2.py'}, {'key': 'nlp/datasets/squad_v2/urls_checksums/checksums.txt', 'etag': '\"db26bffbc79c0ccda031af19d0607518\"', 'lastModified': '2020-05-14T14:57:28.000Z', 'size': 279, 'rfilename': 'urls_checksums/checksums.txt'}]\n",
      "ðŸ‘‰ author: None\n"
     ]
    }
   ],
   "source": [
    "# You can read a few attributes of the datasets before loading them (they are python dataclasses)\n",
    "from dataclasses import asdict\n",
    "\n",
    "for key, value in asdict(datasets[75]).items():\n",
    "    print('ðŸ‘‰ ' + key + ': ' + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:53:45.155432 24912 load.py:154] Checking C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\a09ebf7967b9be046d913d02a0ef7477e90d18055c7449095ada19da5c9e14b6.0277f7f630756bc53bb9c33556b2322d52b731504dd47d995302e1d2d897e9fd.py for additional imports.\n",
      "I0517 22:53:45.159389 24912 filelock.py:274] Lock 1926115877960 acquired on C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\a09ebf7967b9be046d913d02a0ef7477e90d18055c7449095ada19da5c9e14b6.0277f7f630756bc53bb9c33556b2322d52b731504dd47d995302e1d2d897e9fd.py.lock\n",
      "I0517 22:53:45.160405 24912 load.py:317] Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad_v2/squad_v2.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad_v2\n",
      "I0517 22:53:45.161385 24912 load.py:330] Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad_v2/squad_v2.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad_v2\\12c16b15f6ee66809d3ca81ae2eeb4a5c9268a6899888bebf6f042774af8138a\n",
      "I0517 22:53:45.162395 24912 load.py:343] Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad_v2/squad_v2.py to c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad_v2\\12c16b15f6ee66809d3ca81ae2eeb4a5c9268a6899888bebf6f042774af8138a\\squad_v2.py\n",
      "I0517 22:53:45.163378 24912 load.py:351] Couldn't find dataset infos file at https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad_v2\\dataset_infos.json\n",
      "I0517 22:53:45.164375 24912 load.py:364] Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad_v2/squad_v2.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad_v2\\12c16b15f6ee66809d3ca81ae2eeb4a5c9268a6899888bebf6f042774af8138a\\squad_v2.json\n",
      "I0517 22:53:45.166370 24912 filelock.py:318] Lock 1926115877960 released on C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\a09ebf7967b9be046d913d02a0ef7477e90d18055c7449095ada19da5c9e14b6.0277f7f630756bc53bb9c33556b2322d52b731504dd47d995302e1d2d897e9fd.py.lock\n",
      "I0517 22:53:45.171357 24912 builder.py:159] No config specified, defaulting to first: squad_v2/squad_v2\n",
      "I0517 22:53:45.174352 24912 builder.py:132] Overwrite dataset info from restored data version.\n",
      "I0517 22:53:45.176344 24912 info.py:145] Loading Dataset info from C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\n",
      "I0517 22:53:45.182327 24912 builder.py:301] Reusing dataset squad_v2 (C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0)\n",
      "I0517 22:53:45.183325 24912 builder.py:434] Constructing Dataset for split validation[:20%], from C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Downloading and loading a dataset\n",
    "\n",
    "dataset = nlp.load_dataset('squad_v2', split='validation[:20%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(schema: {'id': 'string', 'title': 'string', 'context': 'string', 'question': 'string', 'answers': 'struct<text: list<item: string>, answer_start: list<item: int32>>'}, num_rows: 2375)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': [{'answer_start': [711, 524, 711],\n",
      "              'text': ['Seine', 'Epte', 'Seine']}],\n",
      " 'context': ['In the course of the 10th century, the initially destructive '\n",
      "             'incursions of Norse war bands into the rivers of France evolved '\n",
      "             'into more permanent encampments that included local women and '\n",
      "             'personal property. The Duchy of Normandy, which began in 911 as '\n",
      "             'a fiefdom, was established by the treaty of Saint-Clair-sur-Epte '\n",
      "             'between King Charles III of West Francia and the famed Viking '\n",
      "             'ruler Rollo, and was situated in the former Frankish kingdom of '\n",
      "             'Neustria. The treaty offered Rollo and his men the French lands '\n",
      "             'between the river Epte and the Atlantic coast in exchange for '\n",
      "             'their protection against further Viking incursions. The area '\n",
      "             'corresponded to the northern part of present-day Upper Normandy '\n",
      "             'down to the river Seine, but the Duchy would eventually extend '\n",
      "             'west beyond the Seine. The territory was roughly equivalent to '\n",
      "             'the old province of Rouen, and reproduced the Roman '\n",
      "             'administrative structure of Gallia Lugdunensis II (part of the '\n",
      "             'former Gallia Lugdunensis).'],\n",
      " 'id': ['56dde0ba66d3e219004dad77'],\n",
      " 'question': ['What river originally bounded the Duchy'],\n",
      " 'title': ['Normans']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[23:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In what country is Normandy located?', 'When were the Normans in Normandy?', 'From which countries did the Norse originate?', 'Who was the Norse leader?', 'What century did the Normans first gain their separate identity?']\n"
     ]
    }
   ],
   "source": [
    "# You can get a full column of the dataset by indexing with its name as a string:\n",
    "print(dataset['question'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modifying the dataset with dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main interest of .map() is to update and modify the content of the table and leverage smart caching and fast backend.\n",
    "# To use .map() to update elements in the table you need to provide a function with the following signature: function(example: dict) -> dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:30:23.861944  2632 arrow_dataset.py:528] Caching processed dataset at C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-12f284e1ee175be449097eaf93a75a13.arrow\n",
      "2375it [00:00, 14606.74it/s]\n",
      "I0517 22:30:24.032507  2632 arrow_writer.py:183] Done writing 2375 examples in 2009175 bytes C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-12f284e1ee175be449097eaf93a75a13.arrow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HJNormans', 'HJComputational_complexity_theory', 'HJSouthern_California', 'HJSky_(United_Kingdom)', 'HJVictoria_(Australia)', 'HJHuguenot', 'HJSteam_engine', 'HJOxygen']\n"
     ]
    }
   ],
   "source": [
    "# Let's add a prefix 'HJ to each of our titles\n",
    "\n",
    "def add_prefix_to_title(example):\n",
    "    example['title'] = 'HJ' + example['title']\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_prefix_to_title)\n",
    "\n",
    "print(dataset.unique('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing columns\n",
    "### You can also remove columns when running map with the remove_columns=List[str] argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:38:37.352584  2632 arrow_dataset.py:528] Caching processed dataset at C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-0e2a3bd587a50b06b702864bf09c1828.arrow\n",
      "2375it [00:00, 13230.79it/s]\n",
      "I0517 22:38:37.539085  2632 arrow_writer.py:183] Done writing 2375 examples in 2030550 bytes C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-0e2a3bd587a50b06b702864bf09c1828.arrow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'context', 'question', 'answers', 'new_title']\n",
      "['Wouhahh: HJNormans', 'Wouhahh: HJComputational_complexity_theory', 'Wouhahh: HJSouthern_California', 'Wouhahh: HJSky_(United_Kingdom)', 'Wouhahh: HJVictoria_(Australia)', 'Wouhahh: HJHuguenot', 'Wouhahh: HJSteam_engine', 'Wouhahh: HJOxygen']\n"
     ]
    }
   ],
   "source": [
    "# This will remove the 'title' column while doing the update (after having send it the the mapped function so you can use it in your function!)\n",
    "dataset = dataset.map(lambda example: {'new_title': 'Wouhahh: ' + example['title']},\n",
    "                     remove_columns=['title'])\n",
    "\n",
    "print(dataset.column_names)\n",
    "print(dataset.unique('new_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using examples indices\n",
    "# With with_indices=True, dataset indices (from 0 to len(dataset)) will be supplied to the function which must thus have the following signature: function(example: dict, indice: int) -> dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:40:23.757565  2632 arrow_dataset.py:528] Caching processed dataset at C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-513a9702ea42eb412415c62d4da11924.arrow\n",
      "2375it [00:00, 16086.45it/s]\n",
      "I0517 22:40:23.912139  2632 arrow_writer.py:183] Done writing 2375 examples in 2043690 bytes C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad_v2\\squad_v2\\2.0.0\\cache-513a9702ea42eb412415c62d4da11924.arrow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: In what country is Normandy located?\n",
      "1: When were the Normans in Normandy?\n",
      "2: From which countries did the Norse originate?\n",
      "3: Who was the Norse leader?\n",
      "4: What century did the Normans first gain their separate identity?\n"
     ]
    }
   ],
   "source": [
    "# This will add the index in the dataset to the 'question' field\n",
    "dataset = dataset.map(lambda example, idx: {'question': f'{idx}: ' + example['question']},\n",
    "                      with_indices=True)\n",
    "\n",
    "print('\\n'.join(dataset['question'][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:26.906004 24912 load.py:154] Checking C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\09ec6948d9db29db9a2dcd08df97ac45bccfa6aa104ea62d73c97fa4aaa5cd6c.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py for additional imports.\n",
      "I0517 22:58:26.911950 24912 filelock.py:274] Lock 1924155313864 acquired on C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\09ec6948d9db29db9a2dcd08df97ac45bccfa6aa104ea62d73c97fa4aaa5cd6c.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
      "I0517 22:58:26.914942 24912 load.py:317] Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad/squad.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad\n",
      "I0517 22:58:26.918932 24912 load.py:330] Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad/squad.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad\\c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\n",
      "I0517 22:58:26.921924 24912 load.py:343] Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad/squad.py to c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad\\c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\\squad.py\n",
      "I0517 22:58:26.923916 24912 load.py:351] Couldn't find dataset infos file at https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad\\dataset_infos.json\n",
      "I0517 22:58:26.926910 24912 load.py:364] Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/squad/squad.py at c:\\users\\bokhy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nlp\\datasets\\squad\\c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\\squad.json\n",
      "I0517 22:58:26.928904 24912 filelock.py:318] Lock 1924155313864 released on C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\09ec6948d9db29db9a2dcd08df97ac45bccfa6aa104ea62d73c97fa4aaa5cd6c.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
      "I0517 22:58:26.930896 24912 builder.py:159] No config specified, defaulting to first: squad/plain_text\n",
      "I0517 22:58:26.933893 24912 builder.py:132] Overwrite dataset info from restored data version.\n",
      "I0517 22:58:26.934890 24912 info.py:145] Loading Dataset info from C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\n",
      "I0517 22:58:26.947853 24912 builder.py:301] Reusing dataset squad (C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0)\n",
      "I0517 22:58:26.948884 24912 builder.py:434] Constructing Dataset for split None, from C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\n",
      "I0517 22:58:27.348457 24912 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at C:\\Users\\bokhy/.cache\\torch\\transformers\\5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0517 22:58:27.386310 24912 arrow_dataset.py:518] Loading cached processed dataset at C:\\Users\\bokhy\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\cache-22f66fb97029056f964fa6c0093271ff.arrow\n",
      "I0517 22:58:27.393293 24912 arrow_dataset.py:224] Set __getitem__(key) output type to torch for ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n"
     ]
    }
   ],
   "source": [
    "# Load our training dataset and tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "dataset = nlp.load_dataset('squad')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "def get_correct_alignement(context, answer):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer['text'][0]\n",
    "    start_idx = answer['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx       # When the gold label position is good\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
    "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "# Tokenize our training dataset\n",
    "# To work on batched inputs, set batched=True when calling .map() and supply a function with the following signature\n",
    "def convert_to_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    input_pairs = list(zip(example_batch['context'], example_batch['question']))\n",
    "    encodings = tokenizer.batch_encode_plus(input_pairs, pad_to_max_length=True)\n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
    "        start_idx, end_idx = get_correct_alignement(context, answer)\n",
    "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
    "        end_positions.append(encodings.char_to_token(i, end_idx-1))\n",
    "    encodings.update({'start_positions': start_positions,\n",
    "                      'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "dataset['train'] = dataset['train'].map(convert_to_features, batched=True) # To work on batched inputs\n",
    "\n",
    "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "dataset['train'].set_format(type='torch', columns=columns)\n",
    "\n",
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:42.030625 24912 filelock.py:274] Lock 1926110230472 acquired on C:\\Users\\bokhy/.cache\\torch\\transformers\\774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494.lock\n",
      "I0517 22:58:42.032609 24912 file_utils.py:436] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json not found in cache or force_download set to True, downloading to C:\\Users\\bokhy\\.cache\\torch\\transformers\\tmpwpogsjco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb3a263aefd481a833bbe0e0346e2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:42.435276 24912 file_utils.py:440] storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json in cache at C:\\Users\\bokhy/.cache\\torch\\transformers\\774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494\n",
      "I0517 22:58:42.437271 24912 file_utils.py:443] creating metadata file for C:\\Users\\bokhy/.cache\\torch\\transformers\\774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494\n",
      "I0517 22:58:42.438270 24912 filelock.py:318] Lock 1926110230472 released on C:\\Users\\bokhy/.cache\\torch\\transformers\\774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494.lock\n",
      "I0517 22:58:42.444285 24912 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json from cache at C:\\Users\\bokhy/.cache\\torch\\transformers\\774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494\n",
      "I0517 22:58:42.445284 24912 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:43.064604 24912 filelock.py:274] Lock 1926173697032 acquired on C:\\Users\\bokhy/.cache\\torch\\transformers\\185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d.lock\n",
      "I0517 22:58:43.066599 24912 file_utils.py:436] https://cdn.huggingface.co/distilbert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\bokhy\\.cache\\torch\\transformers\\tmputxndgos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cf58d2432f472c8f67cce8c4b7e060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:53.000996 24912 file_utils.py:440] storing https://cdn.huggingface.co/distilbert-base-cased-pytorch_model.bin in cache at C:\\Users\\bokhy/.cache\\torch\\transformers\\185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d\n",
      "I0517 22:58:53.002989 24912 file_utils.py:443] creating metadata file for C:\\Users\\bokhy/.cache\\torch\\transformers\\185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d\n",
      "I0517 22:58:53.005002 24912 filelock.py:318] Lock 1926173697032 released on C:\\Users\\bokhy/.cache\\torch\\transformers\\185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d.lock\n",
      "I0517 22:58:53.005980 24912 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/distilbert-base-cased-pytorch_model.bin from cache at C:\\Users\\bokhy/.cache\\torch\\transformers\\185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0517 22:58:55.682359 24912 modeling_utils.py:708] Weights of BertForQuestionAnswering not initialized from pretrained model: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "I0517 22:58:55.683357 24912 modeling_utils.py:714] Weights from pretrained model not used in BertForQuestionAnswering: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n"
     ]
    }
   ],
   "source": [
    "# Let's load a pretrained Bert model and a simple optimizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - loss: 6.36\n",
      "Step 1 - loss: 5.39\n",
      "Step 2 - loss: 5.02\n",
      "Step 3 - loss: 5.24\n",
      "Step 4 - loss: 5.12\n"
     ]
    }
   ],
   "source": [
    "# Now let's train our model\n",
    "\n",
    "model.train()\n",
    "for i, batch in enumerate(dataloader):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    print(f'Step {i} - loss: {loss:.3}')\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "\n",
    "# If you only have a single iteration, you can easily compute the score like this\n",
    "predictions = model(inputs)\n",
    "score = bleu_metric.compute(predictions, references)\n",
    " \n",
    "# or    \n",
    "\n",
    "# You need to give the total number of parallel python processes (num_process) and the id of each process (process_id)\n",
    "bleu = nlp.load_metric('bleu', process_id=torch.distributed.get_rank(),b num_process=torch.distributed.get_world_size())\n",
    " \n",
    "# If you have a loop, you can \"add\" your predictions and references at each iteration instead of having to save them yourself (the metric object store them efficiently for you)\n",
    "for batch in dataloader:\n",
    "    model_input, targets = batch\n",
    "    predictions = model(model_inputs)\n",
    "    bleu.add(predictions, targets)\n",
    "score = bleu_metric.compute()  # Compute the score on the first node by default (can be set to compute on each node as well)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
